import os
import argparse
import sys
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from vector_store import get_embeddings, load_vector_store
from llm_loader import load_llama_model

def create_refine_prompts_with_pages(language="ko"):
    if language == "ko":
        question_prompt = PromptTemplate(
            input_variables=["context_str", "question"],
            template="""
ë‹¤ìŒì€ ê²€ìƒ‰ëœ ë¬¸ì„œ ì¡°ê°ë“¤ì…ë‹ˆë‹¤:

{context_str}

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 

**ì¤‘ìš”í•œ ê·œì¹™:**
- ë‹µë³€ ì‹œ ì°¸ê³ í•œ ë¬¸ì„œê°€ ìˆë‹¤ë©´ í•´ë‹¹ ì •ë³´ë¥¼ ì¸ìš©í•˜ì„¸ìš”
- ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”  
- í˜ì´ì§€ ë²ˆí˜¸ë‚˜ ì¶œì²˜ëŠ” ìœ„ ë¬¸ì„œì—ì„œ í™•ì¸ëœ ê²ƒë§Œ ì–¸ê¸‰í•˜ì„¸ìš”
- í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ"ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”

ì§ˆë¬¸: {question}
ë‹µë³€:"""
        )

        refine_prompt = PromptTemplate(
            input_variables=["question", "existing_answer", "context_str"],
            template="""
ê¸°ì¡´ ë‹µë³€:
{existing_answer}

ì¶”ê°€ ë¬¸ì„œ:
{context_str}

ê¸°ì¡´ ë‹µë³€ì„ ìœ„ ì¶”ê°€ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ì™„í•˜ê±°ë‚˜ ìˆ˜ì •í•´ì£¼ì„¸ìš”.

**ê·œì¹™:**
- ìƒˆë¡œìš´ ì •ë³´ê°€ ê¸°ì¡´ ë‹µë³€ê³¼ ë‹¤ë¥´ë‹¤ë©´ ìˆ˜ì •í•˜ì„¸ìš”
- ì¶”ê°€ ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- í•˜ë‚˜ì˜ ì™„ê²°ëœ ë‹µë³€ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
- í™•ì‹¤í•˜ì§€ ì•Šì€ ì¶œì²˜ë‚˜ í˜ì´ì§€ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”

ì§ˆë¬¸: {question}
ë‹µë³€:"""
        )
    else:
        question_prompt = PromptTemplate(
            input_variables=["context_str", "question"],
            template="""
Here are the retrieved document fragments:

{context_str}

Please answer the question based on the above documents.

**Important rules:**
- Only use information explicitly stated in the documents
- If citing sources, only mention what is clearly indicated in the documents above
- Do not guess or infer page numbers not shown in the context
- If unsure, state "not confirmed in the provided documents"

Question: {question}
Answer:"""
        )

        refine_prompt = PromptTemplate(
            input_variables=["question", "existing_answer", "context_str"],
            template="""
Existing answer:
{existing_answer}

Additional documents:
{context_str}

Refine the existing answer using the additional documents.

**Rules:**
- Only use information explicitly stated in the additional documents
- Create one coherent final answer
- Do not mention uncertain sources or page numbers

Question: {question}
Answer:"""
        )

    return question_prompt, refine_prompt

def build_rag_chain(llm, vectorstore, language="ko", k=7):
    """RAG ì²´ì¸ êµ¬ì¶•"""
    question_prompt, refine_prompt = create_refine_prompts_with_pages(language)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="refine",
        retriever=vectorstore.as_retriever(search_kwargs={"k": k}),
        chain_type_kwargs={
            "question_prompt": question_prompt,
            "refine_prompt": refine_prompt
        },
        return_source_documents=True
    )

    return qa_chain

def ask_question_with_pages(qa_chain, question):
    """ì§ˆë¬¸ ì²˜ë¦¬"""
    result = qa_chain.invoke({"query": question})

    # ê²°ê³¼ì—ì„œ A: ì´í›„ ë¬¸ì¥ë§Œ ì¶”ì¶œ
    answer = result['result']
    final_answer = answer.split("A:")[-1].strip() if "A:" in answer else answer.strip()

    print(f"\nğŸ§¾ ì§ˆë¬¸: {question}")
    print(f"\nğŸŸ¢ ìµœì¢… ë‹µë³€: {final_answer}")

    # ë©”íƒ€ë°ì´í„° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ (ë¹„í™œì„±í™”)
    # debug_metadata_info(result["source_documents"])

    # ì°¸ê³  ë¬¸ì„œë¥¼ í˜ì´ì§€ë³„ë¡œ ì •ë¦¬
    print("\nğŸ“š ì°¸ê³  ë¬¸ì„œ ìš”ì•½:")
    source_info = {}
    
    for doc in result["source_documents"]:
        source = doc.metadata.get('source', 'N/A')
        page = doc.metadata.get('page', 'N/A')
        doc_type = doc.metadata.get('type', 'N/A')
        section = doc.metadata.get('section', None)
        total_pages = doc.metadata.get('total_pages', None)
        
        filename = doc.metadata.get('filename', 'N/A')
        if filename == 'N/A':
            filename = os.path.basename(source) if source != 'N/A' else 'N/A'
        
        if filename not in source_info:
            source_info[filename] = {
                'pages': set(), 
                'sections': set(),
                'types': set(),
                'total_pages': total_pages
            }
        
        if page != 'N/A':
            if isinstance(page, str) and page.startswith('ì„¹ì…˜'):
                source_info[filename]['sections'].add(page)
            else:
                source_info[filename]['pages'].add(page)
        
        if section is not None:
            source_info[filename]['sections'].add(f"ì„¹ì…˜ {section}")
        
        source_info[filename]['types'].add(doc_type)

    # ê²°ê³¼ ì¶œë ¥
    total_chunks = len(result["source_documents"])
    print(f"ì´ ì‚¬ìš©ëœ ì²­í¬ ìˆ˜: {total_chunks}")
    
    for filename, info in source_info.items():
        print(f"\n- {filename}")
        
        # ì „ì²´ í˜ì´ì§€ ìˆ˜ ì •ë³´
        if info['total_pages']:
            print(f"  ì „ì²´ í˜ì´ì§€ ìˆ˜: {info['total_pages']}")
        
        # í˜ì´ì§€ ì •ë³´ ì¶œë ¥
        if info['pages']:
            pages_list = list(info['pages'])
            print(f"  í˜ì´ì§€: {', '.join(map(str, pages_list))}")
        
        # ì„¹ì…˜ ì •ë³´ ì¶œë ¥  
        if info['sections']:
            sections_list = sorted(list(info['sections']))
            print(f"  ì„¹ì…˜: {', '.join(sections_list)}")
        
        # í˜ì´ì§€ì™€ ì„¹ì…˜ì´ ëª¨ë‘ ì—†ëŠ” ê²½ìš°
        if not info['pages'] and not info['sections']:
            print(f"  í˜ì´ì§€: ì •ë³´ ì—†ìŒ")
            
        # ë¬¸ì„œ ìœ í˜• ì¶œë ¥
        types_str = ', '.join(sorted(info['types']))
        print(f"  ìœ í˜•: {types_str}")

    return result

# ê¸°ì¡´ ask_question í•¨ìˆ˜ëŠ” ask_question_with_pagesë¡œ êµì²´
def ask_question(qa_chain, question):
    """í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    return ask_question_with_pages(qa_chain, question)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="RAG refine system (í˜ì´ì§€ ë²ˆí˜¸ ì§€ì›)")
    parser.add_argument("--vector_store", type=str, default="vector_db", help="ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œ")
    parser.add_argument("--model", type=str, default="LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct", help="LLM ëª¨ë¸ ID")
    parser.add_argument("--device", type=str, default="cuda", choices=["cuda", "cpu"], help="ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤")
    parser.add_argument("--k", type=int, default=7, help="ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜")
    parser.add_argument("--language", type=str, default="ko", choices=["ko", "en"], help="ì‚¬ìš©í•  ì–¸ì–´")
    parser.add_argument("--query", type=str, help="ì§ˆë¬¸ (ì—†ìœ¼ë©´ ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰)")

    args = parser.parse_args()

    embeddings = get_embeddings(device=args.device)
    vectorstore = load_vector_store(embeddings, load_path=args.vector_store)
    llm = load_llama_model()

    qa_chain = build_rag_chain(llm, vectorstore, language=args.language, k=args.k)

    print("ğŸŸ¢ RAG í˜ì´ì§€ ë²ˆí˜¸ ì§€ì› ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")

    if args.query:
        ask_question_with_pages(qa_chain, args.query)
    else:
        print("ğŸ’¬ ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘ (ì¢…ë£Œí•˜ë ¤ë©´ 'exit', 'quit', 'ì¢…ë£Œ' ì…ë ¥)")
        while True:
            try:
                query = input("\nì§ˆë¬¸: ").strip()
                if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                    break
                if query:  # ë¹ˆ ì…ë ¥ ë°©ì§€
                    ask_question_with_pages(qa_chain, query)
            except KeyboardInterrupt:
                print("\n\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"â— ì˜¤ë¥˜ ë°œìƒ: {e}\në‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
