import os
import re
import glob
import time
from collections import defaultdict

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
    print("âœ… PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    PYMUPDF_AVAILABLE = False
    print("âš ï¸ PyMuPDF ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install PyMuPDFë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

# PDF ì²˜ë¦¬ìš©
import pytesseract
from PIL import Image
from pdf2image import convert_from_path
import pdfplumber
from pymupdf4llm import LlamaMarkdownReader

# --------------------------------
# ë¡œê·¸ ì¶œë ¥
# --------------------------------

def log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")

# --------------------------------
# í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜
# --------------------------------

def clean_text(text):
    return re.sub(r"[^\uAC00-\uD7A3\u1100-\u11FF\u3130-\u318F\w\s.,!?\"'()$:\-]", "", text)

def apply_corrections(text):
    corrections = {
        'ÂºÂ©': 'ì •ë³´', 'ÃŒ': 'ì˜', 'Â½': 'ìš´ì˜', 'Ãƒ': '', 'Â©': '',
        'Ã¢â‚¬â„¢': "'", 'Ã¢â‚¬Å“': '"', 'Ã¢â‚¬': '"'
    }
    for k, v in corrections.items():
        text = text.replace(k, v)
    return text

# --------------------------------
# HWPX ì²˜ë¦¬ (ì„¹ì…˜ë³„ ì²˜ë¦¬ë§Œ ì‚¬ìš©)
# --------------------------------

def load_hwpx(file_path):
    """HWPX íŒŒì¼ ë¡œë”© (XML íŒŒì‹± ë°©ì‹ë§Œ ì‚¬ìš©)"""
    import zipfile
    import xml.etree.ElementTree as ET
    import chardet
    
    log(f"ğŸ“¥ HWPX ì„¹ì…˜ë³„ ì²˜ë¦¬ ì‹œì‘: {file_path}")
    start = time.time()
    documents = []
    
    try:
        with zipfile.ZipFile(file_path, 'r') as zip_ref:
            file_list = zip_ref.namelist()
            section_files = [f for f in file_list 
                           if f.startswith('Contents/section') and f.endswith('.xml')]
            section_files.sort()  # section0.xml, section1.xml ìˆœì„œë¡œ ì •ë ¬
            
            log(f"ğŸ“„ ë°œê²¬ëœ ì„¹ì…˜ íŒŒì¼: {len(section_files)}ê°œ")
            
            for section_idx, section_file in enumerate(section_files):
                with zip_ref.open(section_file) as xml_file:
                    raw = xml_file.read()
                    encoding = chardet.detect(raw)['encoding'] or 'utf-8'
                    try:
                        text = raw.decode(encoding)
                    except UnicodeDecodeError:
                        text = raw.decode("cp949", errors="replace")

                    tree = ET.ElementTree(ET.fromstring(text))
                    root = tree.getroot()
                    
                    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì—†ì´ í…ìŠ¤íŠ¸ ì°¾ê¸°
                    t_elements = [elem for elem in root.iter() if elem.tag.endswith('}t') or elem.tag == 't']
                    body_text = ""
                    for elem in t_elements:
                        if elem.text:
                            body_text += clean_text(elem.text) + " "

                    # page ë©”íƒ€ë°ì´í„°ëŠ” ë¹ˆ ê°’ìœ¼ë¡œ ì„¤ì •
                    page_value = ""

                    if body_text.strip():
                        documents.append(Document(
                            page_content=apply_corrections(body_text),
                            metadata={
                                "source": file_path,
                                "filename": os.path.basename(file_path),
                                "type": "hwpx_body",
                                "page": page_value,
                                "total_sections": len(section_files)
                            }
                        ))
                        log(f"âœ… ì„¹ì…˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (chars: {len(body_text)})")

                    # í‘œ ì°¾ê¸°
                    table_elements = [elem for elem in root.iter() if elem.tag.endswith('}table') or elem.tag == 'table']
                    if table_elements:
                        table_text = ""
                        for table_idx, table in enumerate(table_elements):
                            table_text += f"[Table {table_idx + 1}]\n"
                            rows = [elem for elem in table.iter() if elem.tag.endswith('}tr') or elem.tag == 'tr']
                            for row in rows:
                                row_text = []
                                cells = [elem for elem in row.iter() if elem.tag.endswith('}tc') or elem.tag == 'tc']
                                for cell in cells:
                                    cell_texts = []
                                    for t_elem in cell.iter():
                                        if (t_elem.tag.endswith('}t') or t_elem.tag == 't') and t_elem.text:
                                            cell_texts.append(clean_text(t_elem.text))
                                    row_text.append(" ".join(cell_texts))
                                if row_text:
                                    table_text += "\t".join(row_text) + "\n"
                        
                        if table_text.strip():
                            documents.append(Document(
                                page_content=apply_corrections(table_text),
                                metadata={
                                    "source": file_path,
                                    "filename": os.path.basename(file_path),
                                    "type": "hwpx_table",
                                    "page": page_value,
                                    "total_sections": len(section_files)
                                }
                            ))
                            log(f"ğŸ“Š í‘œ ì¶”ì¶œ ì™„ë£Œ")

                    # ì´ë¯¸ì§€ ì°¾ê¸°
                    if [elem for elem in root.iter() if elem.tag.endswith('}picture') or elem.tag == 'picture']:
                        documents.append(Document(
                            page_content="[ì´ë¯¸ì§€ í¬í•¨]",
                            metadata={
                                "source": file_path,
                                "filename": os.path.basename(file_path),
                                "type": "hwpx_image",
                                "page": page_value,
                                "total_sections": len(section_files)
                            }
                        ))
                        log(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë°œê²¬")
                        
    except Exception as e:
        log(f"âŒ HWPX ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    duration = time.time() - start
    
    # ë¬¸ì„œ ì •ë³´ ìš”ì•½ ì¶œë ¥
    if documents:
        log(f"ğŸ“‹ ì¶”ì¶œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    
    log(f"âœ… HWPX ì²˜ë¦¬ ì™„ë£Œ: {file_path} â±ï¸ {duration:.2f}ì´ˆ, ì´ {len(documents)}ê°œ ë¬¸ì„œ")
    return documents

# --------------------------------
# PDF ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# --------------------------------

def run_ocr_on_image(image: Image.Image, lang='kor+eng'):
    return pytesseract.image_to_string(image, lang=lang)

def extract_images_with_ocr(pdf_path, lang='kor+eng'):
    try:
        images = convert_from_path(pdf_path)
        page_ocr_data = {}
        for idx, img in enumerate(images):
            page_num = idx + 1
            text = run_ocr_on_image(img, lang=lang)
            if text.strip():
                page_ocr_data[page_num] = text.strip()
        return page_ocr_data
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ OCR ì‹¤íŒ¨: {e}")
        return {}

def extract_tables_with_pdfplumber(pdf_path):
    page_table_data = {}
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for i, page in enumerate(pdf.pages):
                page_num = i + 1
                tables = page.extract_tables()
                table_text = ""
                for t_index, table in enumerate(tables):
                    if table:
                        table_text += f"[Table {t_index+1}]\n"
                        for row in table:
                            row_text = "\t".join(cell if cell else "" for cell in row)
                            table_text += row_text + "\n"
                if table_text.strip():
                    page_table_data[page_num] = table_text.strip()
        return page_table_data
    except Exception as e:
        print(f"âŒ í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {}

def extract_body_text_with_pages(pdf_path):
    page_body_data = {}
    try:
        pdf_processor = LlamaMarkdownReader()
        docs = pdf_processor.load_data(file_path=pdf_path)
        
        combined_text = ""
        for d in docs:
            if isinstance(d, dict) and "text" in d:
                combined_text += d["text"]
            elif hasattr(d, "text"):
                combined_text += d.text
        
        if combined_text.strip():
            chars_per_page = 2000
            start = 0
            page_num = 1
            
            while start < len(combined_text):
                end = start + chars_per_page
                if end > len(combined_text):
                    end = len(combined_text)
                
                page_text = combined_text[start:end]
                if page_text.strip():
                    page_body_data[page_num] = page_text.strip()
                    page_num += 1
                
                if end == len(combined_text):
                    break
                start = end - 100
                
    except Exception as e:
        print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    return page_body_data

def load_pdf_with_metadata(pdf_path):
    """PDF íŒŒì¼ì—ì„œ í˜ì´ì§€ë³„ ì •ë³´ë¥¼ ì¶”ì¶œ"""
    log(f"ğŸ“‘ PDF í˜ì´ì§€ë³„ ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
    start = time.time()

    # ë¨¼ì € PyPDFLoaderë¡œ ì‹¤ì œ í˜ì´ì§€ ìˆ˜ í™•ì¸
    try:
        from langchain_community.document_loaders import PyPDFLoader
        loader = PyPDFLoader(pdf_path)
        pdf_pages = loader.load()
        actual_total_pages = len(pdf_pages)
        log(f"ğŸ“„ PyPDFLoaderë¡œ í™•ì¸í•œ ì‹¤ì œ í˜ì´ì§€ ìˆ˜: {actual_total_pages}")
    except Exception as e:
        log(f"âŒ PyPDFLoader í˜ì´ì§€ ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")
        actual_total_pages = 1

    try:
        page_tables = extract_tables_with_pdfplumber(pdf_path)
    except Exception as e:
        page_tables = {}
        print(f"âŒ í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    try:
        page_ocr = extract_images_with_ocr(pdf_path)
    except Exception as e:
        page_ocr = {}
        print(f"âŒ ì´ë¯¸ì§€ OCR ì‹¤íŒ¨: {e}")

    try:
        page_body = extract_body_text_with_pages(pdf_path)
    except Exception as e:
        page_body = {}
        print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    duration = time.time() - start
    log(f"âœ… PDF í˜ì´ì§€ë³„ ì²˜ë¦¬ ì™„ë£Œ: {pdf_path} â±ï¸ {duration:.2f}ì´ˆ")

    # ì‹¤ì œ í˜ì´ì§€ ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
    all_pages = set(page_tables.keys()) | set(page_ocr.keys()) | set(page_body.keys())
    if all_pages:
        max_extracted_page = max(all_pages)
        # ì‹¤ì œ í˜ì´ì§€ ìˆ˜ì™€ ì¶”ì¶œëœ í˜ì´ì§€ ìˆ˜ ì¤‘ í° ê°’ ì‚¬ìš©
        total_pages = max(actual_total_pages, max_extracted_page)
    else:
        total_pages = actual_total_pages

    log(f"ğŸ“Š ìµœì¢… ì„¤ì •ëœ ì´ í˜ì´ì§€ ìˆ˜: {total_pages}")

    docs = []
    
    for page_num in sorted(all_pages):
        if page_num in page_tables and page_tables[page_num].strip():
            docs.append(Document(
                page_content=clean_text(apply_corrections(page_tables[page_num])),
                metadata={
                    "source": pdf_path,
                    "filename": os.path.basename(pdf_path),
                    "type": "table",
                    "page": page_num,
                    "total_pages": total_pages
                }
            ))
            log(f"ğŸ“Š í˜ì´ì§€ {page_num}: í‘œ ì¶”ì¶œ ì™„ë£Œ")
        
        if page_num in page_body and page_body[page_num].strip():
            docs.append(Document(
                page_content=clean_text(apply_corrections(page_body[page_num])),
                metadata={
                    "source": pdf_path,
                    "filename": os.path.basename(pdf_path),
                    "type": "body",
                    "page": page_num,
                    "total_pages": total_pages
                }
            ))
            log(f"ğŸ“„ í˜ì´ì§€ {page_num}: ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ")
        
        if page_num in page_ocr and page_ocr[page_num].strip():
            docs.append(Document(
                page_content=clean_text(apply_corrections(page_ocr[page_num])),
                metadata={
                    "source": pdf_path,
                    "filename": os.path.basename(pdf_path),
                    "type": "ocr",
                    "page": page_num,
                    "total_pages": total_pages
                }
            ))
            log(f"ğŸ–¼ï¸ í˜ì´ì§€ {page_num}: OCR ì¶”ì¶œ ì™„ë£Œ")
    
    if not docs:
        docs.append(Document(
            page_content="[ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨]",
            metadata={
                "source": pdf_path,
                "filename": os.path.basename(pdf_path),
                "type": "error",
                "page": 1,
                "total_pages": total_pages
            }
        ))
    
    # í˜ì´ì§€ ì •ë³´ ìš”ì•½ ì¶œë ¥
    if docs:
        page_numbers = [doc.metadata.get('page', 0) for doc in docs if doc.metadata.get('page')]
        if page_numbers:
            log(f"ğŸ“‹ ì¶”ì¶œëœ í˜ì´ì§€ ë²”ìœ„: {min(page_numbers)} ~ {max(page_numbers)}")
    
    log(f"ğŸ“Š ì¶”ì¶œëœ í˜ì´ì§€ë³„ PDF ë¬¸ì„œ: {len(docs)}ê°œ (ì´ {total_pages}í˜ì´ì§€)")
    return docs

# --------------------------------
# ë¬¸ì„œ ë¡œë”© ë° ë¶„í• 
# --------------------------------

def load_documents(folder_path):
    documents = []

    for file in glob.glob(os.path.join(folder_path, "*.hwpx")):
        log(f"ğŸ“„ HWPX íŒŒì¼ í™•ì¸: {file}")
        docs = load_hwpx(file)
        documents.extend(docs)

    for file in glob.glob(os.path.join(folder_path, "*.pdf")):
        log(f"ğŸ“„ PDF íŒŒì¼ í™•ì¸: {file}")
        documents.extend(load_pdf_with_metadata(file))

    log(f"ğŸ“š ë¬¸ì„œ ë¡œë”© ì „ì²´ ì™„ë£Œ! ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    return documents

def split_documents(documents, chunk_size=800, chunk_overlap=100):
    log("ğŸ”ª ì²­í¬ ë¶„í•  ì‹œì‘")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len
    )
    chunks = []
    for doc in documents:
        split = splitter.split_text(doc.page_content)
        for i, chunk in enumerate(split):
            enriched_chunk = f"passage: {chunk}"
            chunks.append(Document(
                page_content=enriched_chunk,
                metadata={**doc.metadata, "chunk_index": i}
            ))
    log(f"âœ… ì²­í¬ ë¶„í•  ì™„ë£Œ: ì´ {len(chunks)}ê°œ ìƒì„±")
    return chunks

# --------------------------------
# ë©”ì¸ ì‹¤í–‰
# --------------------------------

if __name__ == "__main__":
    folder = "dataset_test"
    log("ğŸš€ PyMuPDF ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘")
    docs = load_documents(folder)
    log("ğŸ“¦ ë¬¸ì„œ ë¡œë”© ì™„ë£Œ")

    # í˜ì´ì§€ ì •ë³´ í™•ì¸
    log("ğŸ“„ í˜ì´ì§€ ì •ë³´ ìš”ì•½:")
    page_info = {}
    for doc in docs:
        source = doc.metadata.get('source', 'unknown')
        page = doc.metadata.get('page', 'unknown')
        doc_type = doc.metadata.get('type', 'unknown')
        
        if source not in page_info:
            page_info[source] = {'pages': set(), 'types': set()}
        page_info[source]['pages'].add(page)
        page_info[source]['types'].add(doc_type)
    
    for source, info in page_info.items():
        max_page = max(info['pages']) if info['pages'] and isinstance(max(info['pages']), int) else 'unknown'
        log(f"  ğŸ“„ {os.path.basename(source)}: {max_page}í˜ì´ì§€, íƒ€ì…: {info['types']}")

    chunks = split_documents(docs)
    log("ğŸ’¡ E5-Large-Instruct ì„ë² ë”© ì¤€ë¹„ ì¤‘")
    embedding_model = HuggingFaceEmbeddings(
        model_name="intfloat/e5-large-v2",
        model_kwargs={"device": "cuda"}
    )

    vectorstore = FAISS.from_documents(chunks, embedding_model)
    vectorstore.save_local("faiss_index_pymupdf_81")

    log(f"ğŸ“Š ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(docs)}")
    log(f"ğŸ”— ì²­í¬ ì´ ìˆ˜: {len(chunks)}")
    log("âœ… FAISS ì €ì¥ ì™„ë£Œ: faiss_index_pymupdf_81")
    
    # í˜ì´ì§€ ì •ë³´ê°€ í¬í•¨ëœ ìƒ˜í”Œ ì¶œë ¥
    log("\nğŸ“‹ ì‹¤ì œ í˜ì´ì§€ ì •ë³´ í¬í•¨ ìƒ˜í”Œ:")
    for i, chunk in enumerate(chunks[:5]):
        meta = chunk.metadata
        log(f"  ì²­í¬ {i+1}: {meta.get('type')} | í˜ì´ì§€ {meta.get('page')} | {os.path.basename(meta.get('source', 'unknown'))}")